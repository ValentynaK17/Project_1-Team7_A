{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bff2f3",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "\n",
    "The dataset used contains data about credit card transctions that occured on two days, Tuesday October 13 and Wednesday October 14 2020.\n",
    "\n",
    "## Part 1 - Read and Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b18716",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'api_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib_venn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m venn3\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Import API key\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapi_keys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m geoapify_key\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Turn off warning messages\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'api_keys'"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import hvplot.pandas\n",
    "import requests\n",
    "import holoviews as hv\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "# Import API key\n",
    "from api_keys import geoapify_key\n",
    "\n",
    "# Turn off warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56168e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "data_df = pd.read_csv(\"Resources/CreditCardData_raw.csv\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows and columns\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11388e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567c6db4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Identify incomplete rows\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data_df\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Identify incomplete rows\n",
    "data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9062020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are only few rows with incomplete information, drop rows with missing data\n",
    "data_df = data_df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dropped rows\n",
    "data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f246b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of each column\n",
    "data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e150fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type for Amount, Date into the appropriate type\n",
    "data_df[\"Amount\"] = data_df[\"Amount\"].str.replace('£', '')\n",
    "data_df[\"Amount\"] = data_df[\"Amount\"].astype(\"float64\")\n",
    "data_df[\"Age\"] = data_df[\"Age\"].astype(\"int64\")\n",
    "data_df[\"Date\"] = pd.to_datetime(data_df[\"Date\"])\n",
    "\n",
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d166e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove # and space from Transaction ID column\n",
    "data_df[\"Transaction ID\"] = data_df[\"Transaction ID\"].str.replace('[# ]', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fe931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that data types have been converted\n",
    "data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ba3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset index\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[data_df[\"Time\"] == 24] = (data_df.loc[data_df[\"Time\"]==24]).replace(24, 0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15321dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[data_df[\"Bank\"] == \"Barlcays\"] = (data_df.loc[data_df[\"Bank\"]==\"Barlcays\"]).replace(\"Barlcays\", \"Barclays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a02ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Amount column\n",
    "data_df = data_df.rename(columns={\"Amount\": \"Amount (in £)\"})\n",
    "data_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2857f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a statistical overview of the numerical columns\n",
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79809c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the IQR and quantitatively determine if there are any potential outliers.\n",
    "\n",
    "trans_type=data_df['Type of Transaction'].unique()\n",
    "amount_list=[]\n",
    "\n",
    "outlier=[]\n",
    "for m in trans_type:\n",
    "     # add subset\n",
    "    amount_data=data_df.loc[data_df[\"Type of Transaction\"] == m,'Amount (in £)']\n",
    "    amount_list.append(amount_data)\n",
    "    quartiles = amount_data.quantile([.25,.5,.75])\n",
    "    lowerq = quartiles[0.25]\n",
    "    upperq = quartiles[0.75]\n",
    "    iqr_t = upperq-lowerq\n",
    "    lower_bound = lowerq - (1.5*iqr_t)\n",
    "    upper_bound = upperq + (1.5*iqr_t)\n",
    "#     print(f\"lower bound = {lower_bound} for {m} \")\n",
    "#     print(f\"upper bound = {upper_bound} for {m} \")\n",
    "    # Determine outliers using upper and lower bounds\n",
    "    outlier=amount_data.loc[(amount_data < lower_bound) | (amount_data > upper_bound)]\n",
    "    if len(outlier)>0:\n",
    "        print (f\"outliers present for {m}\")\n",
    "        print(f\"Values for {m} that are outliers are  {outlier}\")\n",
    "    else:\n",
    "        print(f\"No outliers for {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44dbe1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8088aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(\"Resources/cleaned_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart for Gender distribution with only fraud transactions\n",
    "gender_fraud_counts = fraud_data['Gender'].value_counts()\n",
    "\n",
    "# Increase font size for percentage and make it bold\n",
    "plt.figure(figsize=(8, 8))\n",
    "gender_fraud_counts.plot.pie(autopct='%1.1f%%', startangle=90, colors=['turquoise', 'coral'], textprops={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.title('Gender Distribution for Fraudulent Transactions', fontsize=16, fontweight='bold')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f66add",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['AgeGroup'] = pd.cut(data_df['Age'], bins=[15, 30, 50, 65, 90], labels=[\"<30\", \"30-50\", \"50-65\", \"65-90\"])\n",
    "\n",
    "fraud_data = data_df[data_df['Fraud'] == 1]\n",
    "age_group_fraud_counts = fraud_data['AgeGroup'].value_counts()\n",
    "\n",
    "# Sorting the index to ensure the bar chart is in ascending order\n",
    "age_group_fraud_counts = age_group_fraud_counts.sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = age_group_fraud_counts.plot(kind='bar', color='mediumturquoise')\n",
    "\n",
    "# Making the title and axis labels bold\n",
    "plt.title('Age Group Distribution for Fraudulent Transactions', fontweight='bold')\n",
    "plt.xlabel('Age Group', fontweight='bold')\n",
    "plt.ylabel('Number of Fraudulent Cases', fontweight='bold')\n",
    "\n",
    "# Adding count labels below the bars and making them bold\n",
    "for i, v in enumerate(age_group_fraud_counts):\n",
    "    ax.text(i, v + 0.1, str(v), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e9162d",
   "metadata": {},
   "source": [
    "# At what times of the day is credit card fraud most frequently observed?\n",
    "---\n",
    "Assumption: all the Time mentioned here is from BST timezone (The United Kingdom)\n",
    "\n",
    "### Conclusions:\n",
    " - There are two peeks of fraud during the day:\n",
    "     - 0 AM - 6 AM \n",
    "     - 1 PM\n",
    "  - percentage of the amount of fraud transactions out of the total transaction amount for a specific time of the day is lower than the percentage of the fraud transaction count out of the total transaction count for the same time of the day, which could mean that on average fraud transactions are of a lower value than other transactions\n",
    "  - Fraud Count correlate with Fraud Amount, which mean that on average the values of fraud transaction for a (Day X Time) is more or less consistent, which could mean in its turn that there might be some specific segment of profiles affected the most. The slope here is 0.5, which can help us to predict the increase of one parameter using the other, e.g. 1% increase in the fraud transactions count would give increase in fraud amount 0.5%. Also we can use this info to concentrate on either of the above metric rather then on both of them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = Path(\"Resources/cleaned_data.csv\")\n",
    "cleaned_data = pd.read_csv(f_path, index_col=0)\n",
    "\n",
    "# leave only Tuesday and Wednesday\n",
    "cleaned_data=cleaned_data[(cleaned_data[\"Day of Week\"]==\"Tuesday\")|(cleaned_data[\"Day of Week\"]==\"Wednesday\")]\n",
    "# and have one df for a fraud data only\n",
    "data_fraud=cleaned_data[cleaned_data[\"Fraud\"]==1].copy().reset_index(drop=True)\n",
    "data_fraud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraud[\"Amount (in £)\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8170ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraud[\"Time\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ff59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while analyzing amount, we will take mode per (time X day), as far as data are obviously skewed\n",
    "# source for mode calculation: https://stackoverflow.com/questions/50208007/calculate-mode-of-a-column-in-pandas-using-other-column-with-same-row-values \n",
    "fraud_mode=data_fraud.groupby([\"Day of Week\", \"Time\"]).agg({\"Amount (in £)\": lambda x: x.value_counts().index[0]}).reset_index()   \n",
    "fraud_mode=fraud_mode.rename(columns={\"Amount (in £)\":\"Amount Mode\"})\n",
    "# count transactions per (time X Day), etc.\n",
    "fraud_count=pd.DataFrame(data_fraud.groupby([\"Day of Week\", \"Time\"]).size()).rename(columns={0:\"Count\"}).reset_index()\n",
    "fraud_over_time=pd.merge(fraud_mode,fraud_count, on=[\"Day of Week\",\"Time\"], how=\"outer\")\n",
    "\n",
    "fraud_max=pd.DataFrame(data_fraud.groupby([\"Day of Week\", \"Time\"])[\"Amount (in £)\"].max()).rename(columns={\"Amount (in £)\":\"Amount Max\"}).reset_index()\n",
    "fraud_avg=pd.DataFrame(data_fraud.groupby([\"Day of Week\", \"Time\"])[\"Amount (in £)\"].mean()).rename(columns={\"Amount (in £)\":\"Amount Avg\"}).reset_index()\n",
    "fraud_over_time=pd.merge(fraud_over_time, fraud_max, on=[\"Day of Week\",\"Time\"], how=\"outer\")\n",
    "fraud_over_time=pd.merge(fraud_over_time, fraud_avg, on=[\"Day of Week\",\"Time\"], how=\"outer\")\n",
    "\n",
    "fraud_over_time.sort_values(by=\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "fig, ax = plt.subplots()\n",
    "#plotting two lines for two subsets: Tuesday and Wednesday\n",
    "subset=fraud_over_time[fraud_over_time[\"Day of Week\"]==\"Tuesday\"]\n",
    "ax.plot(subset[\"Time\"], subset[\"Count\"], color=\"springgreen\",label=\"Count Tuesday\")\n",
    "\n",
    "subsetW=fraud_over_time[fraud_over_time[\"Day of Week\"]==\"Wednesday\"]\n",
    "ax.plot(subsetW[\"Time\"], subsetW[\"Count\"], color=\"green\",label=\"Count Wednesday\")\n",
    "tick_loc=subsetW[\"Time\"].unique()\n",
    "# setting labels for x-axis\n",
    "labels_t=[\"12 AM\"]\n",
    "for i in range(1,12): \n",
    "    labels_t.append(f\"{i} AM\")\n",
    "labels_t.append(\"12 PM\")\n",
    "for i in range(1,12): \n",
    "    labels_t.append(f\"{i} PM\")\n",
    "ax.set_xticks(range(0,24))\n",
    "ax.set_xticklabels(labels_t, rotation=45)\n",
    "\n",
    "ax.set_ylabel(\"Number of Transactions\")\n",
    "ax.set_xlabel(\"Time of the Day\")\n",
    "ax.set_title(\"Transactions Count Over Time\")\n",
    "ax.legend( loc=\"best\",prop={'size': 6})\n",
    "ax.grid(alpha=0.2)\n",
    "ax.set_xlim(-0.7,23.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea081d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming that we could have lots of days let us look at more generic picture, using averages\n",
    "# and compare data for amount vs count over time\n",
    "# split amount into few size bins\n",
    "size_bins = [0, 30, 100, 200, 300, 400]\n",
    "labels = [\"Fraud amount in range (0,30]\", \"Fraud amount in range (30,100]\", \"Fraud amount in range (100,200]\", \"Fraud amount in range (200,300]\", \"Fraud amount in range (300,400]\"]\n",
    "colors=[\"yellowgreen\", \"gold\", \"orange\", \"red\", \"maroon\"]  #https://matplotlib.org/stable/gallery/color/named_colors.html\n",
    "cleaned_data[\"Fraud Size\"] = pd.cut(cleaned_data[\"Amount (in £)\"], bins=size_bins, labels=labels)\n",
    "\n",
    "data_stecked=cleaned_data.loc[cleaned_data[\"Fraud\"]==1,[\"Time\", \"Fraud Size\"]].copy()\n",
    "data_stecked=data_stecked.groupby([\"Time\", \"Fraud Size\"]).size()\n",
    "data_stecked=pd.DataFrame(data_stecked).rename(columns={0:\"Size Count\"}).reset_index()\n",
    "\n",
    "\n",
    "#find averages\n",
    "day_count=cleaned_data[\"Date\"].nunique()\n",
    "\n",
    "for i, row in data_stecked.iterrows():\n",
    "    data_stecked.loc[i,\"Size Count\"]=row[\"Size Count\"]/day_count\n",
    "\n",
    "# switch Fraud Size column to a row; source: https://www.w3resource.com/pandas/dataframe/dataframe-pivot.php\n",
    "data_stecked = data_stecked.pivot(index='Time', columns='Fraud Size', values='Size Count').fillna(0)\n",
    "\n",
    "# create shared figuer and x axis\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# Create the stacked bar chart; source: https://www.geeksforgeeks.org/create-a-stacked-bar-plot-in-matplotlib/\n",
    "data_stecked.plot(kind='bar', stacked=True, ax=ax, color=colors, figsize=(9, 7), alpha=0.7, legend=False)\n",
    "\n",
    "#function for calculating the rate\n",
    "def perc(df, col1, col2, res_col1, res_col2):\n",
    "    df[res_col1]=\"\"\n",
    "    df[res_col2]=\"\"\n",
    "    for i, row in df.iterrows():\n",
    "        sum_c=row[col1]+row[col2]\n",
    "        df.loc[i,res_col1]=100*row[col1]/sum_c\n",
    "        df.loc[i,res_col2]=100*row[col2]/sum_c\n",
    "    return df\n",
    "\n",
    "area_df1=cleaned_data.groupby([\"Time\",\"Fraud\"]).size()\n",
    "area_df1=pd.DataFrame(area_df1).reset_index().rename(columns={0:\"Transaction Count\"}).set_index(\"Time\")\n",
    "area_df1=area_df1.pivot(columns='Fraud', values='Transaction Count').fillna(0)\n",
    "area_df1=perc(area_df1, 0,1,\"Genuine Count %\",\"Fraud Count %\")\n",
    "\n",
    "area_df2=cleaned_data.groupby([\"Time\",\"Fraud\"])[\"Amount (in £)\"].sum()\n",
    "area_df2=pd.DataFrame(area_df2).reset_index().rename(columns={\"Amount (in £)\":\"Transaction Amount\"}).set_index(\"Time\")\n",
    "area_df2=area_df2.pivot(columns='Fraud', values='Transaction Amount').fillna(0)\n",
    "area_df2=perc(area_df2, 0,1,\"Genuine Amount %\",\"Fraud Amount %\")\n",
    "\n",
    "ax2=ax.twinx() # magic for having two x axis; source: https://samchaaa.medium.com/how-to-plot-two-different-scales-on-one-plot-in-matplotlib-with-legend-46554ba5915a\n",
    "l1=area_df2.plot(y=[\"Fraud Amount %\"], ax=ax2, color=\"violet\", legend=False)\n",
    "area_df1.plot(y=[\"Fraud Count %\"], ax=ax2, color=\"blue\", legend=False) \n",
    "# setting label per each line; source: https://stackoverflow.com/questions/64111555/get-lines-and-labels-from-matplotlib-axes\n",
    "l1.get_lines()[0].set_label(\"% of Fraud Amount out of Sum\")\n",
    "l1.get_lines()[1].set_label(\"% of Fraud Transactions Number out of Count\")\n",
    "\n",
    "\n",
    "ax.set_ylabel(\"Average Number of Transaction (Time X Date)\")\n",
    "ax2.set_ylabel(\"% out of all the Transactions\")\n",
    "ax.set_xlabel(\"Time of the Day\")\n",
    "ax.set_xticks(range(0,24))\n",
    "ax.set_xticklabels(labels_t, rotation=45)\n",
    "ax.set_title(\"Fraud Distributions Over Time of the Day\")\n",
    "\n",
    "# add legend for all charts https://stackoverflow.com/questions/5484922/secondary-axis-with-twinx-how-to-add-to-legend\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc=\"best\",prop={'size': 6})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13315249",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'area_df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# count and amount seem to go together, so let us look at correlation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_merged_perc\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mmerge(area_df1[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenuine Count \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraud Count \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m]],area_df2[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenuine Amount \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraud Amount \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m]], left_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m df_merged_perc\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m      4\u001b[0m df_merged_perc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraud Count \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mdf_merged_perc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraud Count \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'area_df1' is not defined"
     ]
    }
   ],
   "source": [
    "# count and amount seem to go together, so let us look at correlation\n",
    "df_merged_perc=pd.merge(area_df1[[\"Genuine Count %\",\"Fraud Count %\"]],area_df2[[\"Genuine Amount %\",\"Fraud Amount %\"]], left_index=True, right_index=True)\n",
    "df_merged_perc.reset_index()\n",
    "df_merged_perc[\"Fraud Count %\"]=df_merged_perc[\"Fraud Count %\"].astype(\"float\")\n",
    "df_merged_perc[\"Fraud Amount %\"]=df_merged_perc[\"Fraud Amount %\"].astype(\"float\")\n",
    "df_merged_perc=df_merged_perc[~(df_merged_perc[\"Fraud Count %\"]==100)]\n",
    "df_merged_perc\n",
    "# Define a function to create Linear Regression plots\n",
    "def lin_reg(x, y, x_label, y_label):\n",
    "    slope, b, lin_corr, _, _ = st.linregress(x,y)\n",
    "    pred=slope*x+b\n",
    "    \n",
    "    #print r-value\n",
    "    print(f\"The r-value is:{lin_corr}\")\n",
    "\n",
    "    # Create line equation string\n",
    "    if abs(round(slope,2))>0: eq=f\"y = {round(slope,2)}x +{round(b,2)}\"\n",
    "    else: #we want to omit y=0.0x+b situation\n",
    "        non_zero_decimal_position=2\n",
    "        decimals=str(slope).split(\".\")[1] #https://www.softwaretestinghelp.com/python/python-string-split/\n",
    "        i=decimals[2] #https://buzzcoder.gitbooks.io/codecraft-python/content/string/loop-through-a-string.html\n",
    "        while i=='0': \n",
    "            non_zero_decimal_position+=1\n",
    "            i=decimals[non_zero_decimal_position]\n",
    "        eq=f\"y = {round(slope,non_zero_decimal_position+1)}x +{round(b,2)}\"\n",
    "        \n",
    "    #plot the above regression line along with x_column_name vs y_column_name dependency\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    #plot\n",
    "    ax.scatter(x=x,y=y, marker=\"o\", edgecolor=\"black\", label=\"Original Data\")\n",
    "    ax.plot(x,pred, color=\"fuchsia\", label=\"Linear Regression Prediction\")\n",
    "    #annotation\n",
    "    x_p=x.max()-(x.max()-x.min())/2\n",
    "    y_p=slope*x_p+b+(y.max()-y.min())/50\n",
    "    ax.annotate(eq,(x_p,y_p),fontsize=14, fontweight=\"bold\", color=\"fuchsia\", bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "    #labels...\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(f\"{y_label}\")\n",
    "    ax.set_title(f\"{x_label} vs {y_label}\")  \n",
    "    ax.legend(loc=\"best\",prop={'size': 6})\n",
    "    plt.tight_layout()\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "lin_reg(df_merged_perc[\"Fraud Count %\"],df_merged_perc[\"Fraud Amount %\"],\"Fraud Count %\", \"Fraud Amount %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us very briefly check distributions of different paramenters over the day\n",
    "\n",
    "#create a function for building of bar charts\n",
    "def stacked_bars(df, x_column, y_column, y_values, ax):\n",
    "    df_stecked=df.loc[:,[x_column, y_column]].copy()\n",
    "    df_stecked=df_stecked.groupby([x_column, y_column]).size()\n",
    "    df_stecked=pd.DataFrame(df_stecked).rename(columns={0:y_values}).reset_index()\n",
    "  # switch Fraud Size column to a row\n",
    "    df_stecked = df_stecked.pivot(index=x_column, columns=y_column, values=y_values).fillna(0)\n",
    "  # Create the stacked bar chart\n",
    "    df_stecked.plot(kind='bar', stacked=True, ax=ax, alpha=0.5)\n",
    "    ax.tick_params(axis='x', labelsize=4)\n",
    "    ax.set_title(f\"{y_column} distribution within Fraud Transactions over time\", size=9)\n",
    "    ax.set_ylabel(\"Number of Transactions\", size=6)\n",
    "    ax.set_xlabel(\"Time of the Day\", size=6)\n",
    "    ax.legend(loc=\"best\",prop={'size': 5})\n",
    "\n",
    "#working with only fraud data, so will use data_fraud\n",
    "fig, ax = plt.subplots(8, 1, figsize=(7, 15))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# building stacked bars:\n",
    "stacked_bars(data_fraud, \"Time\", \"Type of Card\", \"Count of Transactions\", ax[0])\n",
    "stacked_bars(data_fraud, \"Time\", \"Entry Mode\", \"Count of Transactions\",  ax[1])\n",
    "stacked_bars(data_fraud, \"Time\", \"Type of Transaction\", \"Count of Transactions\",  ax[2])\n",
    "stacked_bars(data_fraud, \"Time\", \"Merchant Group\", \"Count of Transactions\",  ax[3])\n",
    "stacked_bars(data_fraud, \"Time\", \"Bank\", \"Count of Transactions\",  ax[4])\n",
    "stacked_bars(data_fraud, \"Time\", \"Country of Transaction\", \"Count of Transactions\",  ax[5])\n",
    "stacked_bars(data_fraud, \"Time\", \"Shipping Address\", \"Count of Transactions\",  ax[6])\n",
    "stacked_bars(data_fraud, \"Time\", \"Country of Residence\", \"Count of Transactions\",  ax[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cfb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us quickly look at correlations for numerical fields within fraud transactions vs for the whole set of data\n",
    "data_fraud.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98223d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraud[[\"Time\",\"Amount (in £)\",\"Age\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6baf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data[[\"Time\",\"Amount (in £)\",\"Age\", \"Fraud\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd069321",
   "metadata": {},
   "source": [
    "# Project 1 \n",
    "## Which countries are most frequently associated with Fraud Credit Card transactions within our dataset?\n",
    "\n",
    "Our assumed client from UK asked to investigate 2 days transaction data (close to 10 000 transactions) gathered from 7 institutions to find out which countries are most frequently associated with Fraud transactions and if there are any patterns within Fraud transactions related to country information.\n",
    "\n",
    "#### Conclusions:\n",
    " - About 98% of Fraud transactions associated with UK recidence, however, around 83% of all Fraud transactions have different Country, mentioned within Transaction and Shipping Addresses (with actually same Transaction and Shipping). Almost all the rest fraud transactions (around 16%) have same Recidence and Transaction, but different Shipping address. This pattern is not applicable to genuine transactions with 83% of transactions with same Shipping, Recidence and Transaction adresses.\n",
    " - Having the above in mind we can assume that for fraud transactions country of shipping can be treated as a fraud initiator. This results in final conclusion that there is 1.7% of fraud associated with UK and the rest is split into 4 countries almost equally (24-25%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e7fbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib-venn in c:\\users\\darzi\\anaconda3\\lib\\site-packages (0.11.9)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from matplotlib-venn) (3.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from matplotlib-venn) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from matplotlib-venn) (1.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from matplotlib->matplotlib-venn) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from matplotlib->matplotlib-venn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from matplotlib->matplotlib-venn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from matplotlib->matplotlib-venn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from matplotlib->matplotlib-venn) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from matplotlib->matplotlib-venn) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from matplotlib->matplotlib-venn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\darzi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install matplotlib-venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55043eb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#concentrate on transactions count analysis per country so far\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data_country\u001b[38;5;241m=\u001b[39mcleaned_data[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry of Transaction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShipping Address\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry of Residence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraud\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmount (in £)\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m data_country_fraud\u001b[38;5;241m=\u001b[39mdata_country[data_country[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraud\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m data_country_fraud\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_data' is not defined"
     ]
    }
   ],
   "source": [
    "#concentrate on transactions count analysis per country so far\n",
    "data_country=cleaned_data[[\"Country of Transaction\", \"Shipping Address\", \"Country of Residence\", \"Fraud\", \"Amount (in £)\"]].copy().reset_index(drop=True)\n",
    "data_country_fraud=data_country[data_country[\"Fraud\"]==1].copy()\n",
    "data_country_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a703066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create a function for building of pie charts\n",
    "def pies (groupped_y, ax, color_mapping, legend, explode, start_angle):\n",
    "    y_name=groupped_y.name\n",
    "    colors = [color_mapping[country] for country in groupped_y.index]\n",
    "    groupped_y.plot(kind=\"pie\",autopct=\"%1.1f%%\",labels=groupped_y.index, \n",
    "                    ylabel=y_name, ax=ax, colors=colors, legend=legend,\n",
    "                    shadow=True, explode=explode, startangle=start_angle)\n",
    "    ax.set_title(f\"{y_name} distribution within Fraud Transactions\", size=9)\n",
    "    ax.set_ylabel(y_name, size=8)\n",
    "    ax.axis(\"equal\")\n",
    "    \n",
    "# preparing subplots object\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "# share same color per Country per diff charts; source: https://github.com/geopandas/geopandas/issues/1269\n",
    "color_mapping = {\"USA\": \"slategrey\", \"China\": \"coral\", \"India\": \"orange\", \"Russia\": \"firebrick\", \"United Kingdom\": \"cornflowerblue\"}\n",
    "\n",
    "# building pie charts:\n",
    "temp=data_country_fraud[\"Country of Transaction\"].value_counts()\n",
    "pies(temp, ax[0], color_mapping, False, (0.1,0,0,0,0), 90)\n",
    "temp=data_country_fraud[\"Shipping Address\"].value_counts()                        \n",
    "pies(temp, ax[1], color_mapping, False, (0.1,0,0,0,0), 90)\n",
    "temp=data_country_fraud[\"Country of Residence\"].value_counts()                         \n",
    "pies(temp, ax[2], color_mapping, False, (0.2,0,0,0,0), 90)\n",
    "plt.legend(loc=\"upper right\",prop={'size': 8})\n",
    "# adjust spasing between subplots; source:  https://www.geeksforgeeks.org/how-to-set-the-spacing-between-subplots-in-matplotlib-in-python/\n",
    "plt.subplots_adjust(left=0.7,\n",
    "                    bottom=0.3,\n",
    "                    right=2.5,\n",
    "                    top=1,\n",
    "                    wspace=0.5,\n",
    "                    hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461bb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us group data to check if there is something interesting regarding address\n",
    "combinations_counts=data_country_fraud.groupby([\"Country of Transaction\", \"Shipping Address\", \"Country of Residence\"]).size()\n",
    "combinations_counts=pd.DataFrame(combinations_counts).reset_index()\n",
    "combinations_counts=combinations_counts.rename(columns={0:\"Fraud Transactions Count\"})\n",
    "combinations_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87efe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is obviously a pattern in the above when try separately, \n",
    "    # so let us visualize it with venn diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d3862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def venn_d(df, set1_column_name, set2_column_name, set3_column_name, colors, type_t):\n",
    "    combinations_counts=df.groupby([set1_column_name, set2_column_name, set3_column_name]).size()\n",
    "    combinations_counts=pd.DataFrame(combinations_counts).reset_index()\n",
    "    count_column_name=f\"{type_t} Transactions Count\"\n",
    "    combinations_counts=combinations_counts.rename(columns={0:count_column_name})\n",
    "    # let us prepare numbers of overlapped vs not overlapped data for venn diagram \n",
    "\n",
    "    combinations_counts[\"Transaction=Shipping\"]=[0 for i in combinations_counts.index]\n",
    "    combinations_counts[\"Shipping=Residence\"]=[0 for i in combinations_counts.index]\n",
    "    combinations_counts[\"Transaction=Residence\"]=[0 for i in combinations_counts.index]\n",
    "    combinations_counts[\"Transaction=Shipping=Residence\"]=[0 for i in combinations_counts.index]\n",
    "    combinations_counts[\"No overlap\"]=[0 for i in combinations_counts.index]\n",
    "\n",
    "\n",
    "    for i, row in combinations_counts.iterrows():\n",
    "        if ((row[\"Country of Transaction\"] == row[\"Country of Residence\"])&(row[\"Country of Transaction\"] == row[\"Shipping Address\"])):combinations_counts.loc[i,\"Transaction=Shipping=Residence\"]+=row[count_column_name]\n",
    "        elif row[\"Country of Transaction\"]== row[\"Shipping Address\"]: combinations_counts.loc[i,\"Transaction=Shipping\"]+=row[count_column_name]\n",
    "        elif row[\"Country of Transaction\"]== row[\"Country of Residence\"]: combinations_counts.loc[i,\"Transaction=Residence\"]+=row[count_column_name]\n",
    "        elif row[\"Shipping Address\"]== row[\"Country of Residence\"]:combinations_counts.loc[i,\"Shipping=Residence\"]+=row[count_column_name]\n",
    "        else:   combinations_counts.loc[i,\"No overlap\"]+=row[count_column_name]\n",
    "    combinations_counts  \n",
    "\n",
    "    # few more preparation of data for venn diagram; \n",
    "    # sources for how to build venn: \n",
    "    # - https://www.geeksforgeeks.org/how-to-create-and-customize-venn-diagrams-in-python/\n",
    "    # - https://stackoverflow.com/questions/19841535/python-matplotlib-venn-diagram\n",
    "    totals=combinations_counts[count_column_name].sum()\n",
    "    only=round(100*(combinations_counts[\"No overlap\"].sum())/totals,1)\n",
    "    RS=round(100*(combinations_counts[\"Shipping=Residence\"].sum())/totals,1)\n",
    "    RT=round(100*(combinations_counts[\"Transaction=Residence\"].sum())/totals,1)\n",
    "    ST=round(100*(combinations_counts[\"Transaction=Shipping\"].sum())/totals,1)\n",
    "    RST=round(100*(combinations_counts[\"Transaction=Shipping=Residence\"].sum())/totals,1)\n",
    "\n",
    "    # order: Only r, Only s, r ∩ s, Only t, r ∩ t, s ∩ t, r ∩ s ∩ t\n",
    "    v = venn3(subsets=(only,only,RS,only,RT,ST,RST),set_labels=('Residence','Shipping','Transaction'), set_colors=(colors[0], colors[1], colors[2]))\n",
    "    v.get_label_by_id('A').set_color(colors[0])\n",
    "    v.get_label_by_id('B').set_color(colors[1])\n",
    "    v.get_label_by_id('C').set_color(colors[2])\n",
    "    plt.title(f\"Overlap of Countries in {type_t} Transactions: Residence, Shipping, Transaction (%)\", size=9)\n",
    "\n",
    "    plt.show()\n",
    "    return combinations_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=(\"red\", \"cornflowerblue\", \"grey\")\n",
    "venn_d(data_country_fraud, \"Country of Transaction\", \"Shipping Address\", \"Country of Residence\", colors, \"Fraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2899fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us check same for genuine transactions\n",
    "data_country=cleaned_data[[\"Country of Transaction\", \"Shipping Address\", \"Country of Residence\", \"Fraud\", \"Amount (in £)\"]].copy().reset_index(drop=True)\n",
    "data_country_gen=data_country[data_country[\"Fraud\"]==0].copy()\n",
    "venn_d(data_country_gen, \"Country of Transaction\", \"Shipping Address\", \"Country of Residence\", colors, \"Genuine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc554704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting to investigate shipped address a little more\n",
    "# for this purpose build a distribution of a transactions countries records by shipped countries records\n",
    "stacked_df=combinations_counts.groupby([\"Country of Transaction\",\"Shipping Address\"])[\"Fraud Transactions Count\"].sum()\n",
    "stacked_df=stacked_df.reset_index()\n",
    "stacked_df=stacked_df.pivot(index=\"Country of Transaction\", columns=\"Shipping Address\", values=\"Fraud Transactions Count\").fillna(0)\n",
    "stacked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "stacked_df.plot(kind='barh', stacked=True, ax=ax, alpha=0.5)\n",
    "ax.set_title(\"Country of Transactions by Country of Shipping\", size=9)\n",
    "ax.set_ylabel(\"Country of Transaction\", size=8)\n",
    "ax.set_xlabel(\"Number of Transactions\", size=8)\n",
    "ax.legend(loc=\"best\",prop={'size': 7})\n",
    "plt.grid(axis='x', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7941fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify % of transactions for each country taking into account shipping address specific\n",
    "#prep list of countries\n",
    "countries=stacked_df.columns\n",
    "countries.name=\"Country\"\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728690f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us calculate percentage of transactions per Country\n",
    "def country_perc(df,type_t,countries,step_x):\n",
    "    temp_df=pd.DataFrame(countries).set_index(\"Country\")\n",
    "    count_column_name=f\"{type_t} Transactions Count\"\n",
    "    temp_df[count_column_name]=\"\"\n",
    "    for i in df.columns:\n",
    "        temp_df.loc[i,count_column_name]=df[i].sum()\n",
    "    total_F= temp_df[count_column_name].sum()   \n",
    "    map_df_p=temp_df.copy()\n",
    "    perc_column_name=f\"{type_t} Transactions (%)\"\n",
    "    map_df_p[perc_column_name]=[round(100*i/total_F,1) for i in map_df_p[count_column_name]]\n",
    "    map_df_p=map_df_p.drop(count_column_name, axis=1)\n",
    "\n",
    "    #visualize the above %\n",
    "    max_y=int(map_df_p[perc_column_name].max())+1\n",
    "    labels_t=[]\n",
    "    for i in range(0,max_y,step_x): \n",
    "        labels_t.append(f\"{i} %\")\n",
    "    ax = map_df_p.plot(kind='barh')\n",
    "    ax.set_xticks(range(0,max_y,step_x))\n",
    "\n",
    "    ax.set_xticklabels(labels_t, ha='center', va='top', rotation=45, size=7) \n",
    "    ax.set_xlim(-0.25, max_y+0.1)\n",
    "    plt.title(f\"Contries, associated as {type_t} initiator (%)\")\n",
    "    plt.legend(loc=\"best\", prop={'size': 6})\n",
    "    plt.grid(axis='x', alpha=0.1)\n",
    "    plt.figure(figsize=(5, 25))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return map_df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the Country share within Fraud data\n",
    "fraud_perc=country_perc(stacked_df,\"Fraud\",countries,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e40f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for genuine set\n",
    "data_country=cleaned_data[[\"Country of Transaction\", \"Shipping Address\", \"Country of Residence\", \"Fraud\", \"Amount (in £)\"]].copy().reset_index(drop=True)\n",
    "data_country_gen=data_country[data_country[\"Fraud\"]==0].copy()\n",
    "combinations_counts=data_country_gen.groupby([\"Country of Transaction\", \"Shipping Address\", \"Country of Residence\"]).size()\n",
    "combinations_counts=pd.DataFrame(combinations_counts).reset_index()\n",
    "combinations_counts=combinations_counts.rename(columns={0:\"Genuine Transactions Count\"})\n",
    "ship_cleaned=combinations_counts.groupby([\"Country of Transaction\",\"Shipping Address\"])[\"Genuine Transactions Count\"].sum()\n",
    "ship_cleaned=ship_cleaned.reset_index()\n",
    "ship_cleaned=ship_cleaned.pivot(index=\"Country of Transaction\", columns=\"Shipping Address\", values=\"Genuine Transactions Count\").fillna(0)\n",
    "\n",
    "# ax.set_xlim(-0.25, max_y+0.25)\n",
    "# ax.set_xlim(-0.25, max_y-0.5)\n",
    "\n",
    "gen_perc=country_perc(ship_cleaned,\"Genuine\",countries,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data for overall view\n",
    "ship_merged=pd.merge(gen_perc, fraud_perc, on=\"Country\")\n",
    "ship_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep data for a line with % of Fraud by Country within the whole set of transactions\n",
    "df_prop=cleaned_data[[\"Shipping Address\",\"Fraud\"]]\n",
    "distribution_c=df_prop.groupby([\"Shipping Address\",\"Fraud\"]).size()\n",
    "distribution_c=pd.DataFrame(distribution_c).reset_index().rename(columns={0:\"transactions_count\"})\n",
    "distribution_c = distribution_c.pivot(index=\"Shipping Address\", columns=\"Fraud\", values=\"transactions_count\").fillna(0)\n",
    "distribution_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832dbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion=pd.DataFrame(countries).set_index(\"Country\")\n",
    "proportion[\"%Fraud\"]=\"\"\n",
    "for i,row in distribution_c.iterrows():\n",
    "    proportion.loc[i,\"%Fraud\"]=round(100*row[1]/(row[0]+row[1]),2)\n",
    "proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and plot alltogether\n",
    "# groupped bar charts; source: https://www.pythoncharts.com/matplotlib/grouped-bar-charts-matplotlib/\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "bar_width = 0.4\n",
    "length_x=len(ship_merged[\"Genuine Transactions (%)\"])\n",
    "x = np.arange(length_x)\n",
    "ax.bar(x-bar_width/2,ship_merged[\"Genuine Transactions (%)\"], alpha=0.4, width=bar_width, label=\"Genuine Transaction Distribution by Country (%)\")\n",
    "ax.bar(x+bar_width/2, ship_merged[\"Fraud Transactions (%)\"], width=bar_width, label=\"Fraud Transaction Distribution by Country (%)\") \n",
    "ax.set_xticks(x)\n",
    "labls=ship_merged.index\n",
    "ax.set_xticklabels(labls, ha='center', va='top', rotation=45, size=7)\n",
    "ax.set_xlim(-0.85, length_x-0.25)\n",
    "ax.grid(axis='y', alpha=0.2)\n",
    "\n",
    "for bar in ax.patches: #source: https://www.pythoncharts.com/matplotlib/grouped-bar-charts-matplotlib/\n",
    "  # The text annotation for each bar should be its height.\n",
    "  bar_value = bar.get_height()\n",
    "  # Format the text with commas to separate thousands. You can do\n",
    "  # any type of formatting here though.\n",
    "  text = f'{bar_value:,}'\n",
    "  # This will give the middle of each bar on the x-axis.\n",
    "  text_x = bar.get_x() + bar.get_width() / 2\n",
    "  # get_y() is where the bar starts so we add the height to it.\n",
    "  text_y = bar.get_y() + bar_value\n",
    "  # If we want the text to be the same color as the bar, we can\n",
    "  # get the color like so:\n",
    "  bar_color = bar.get_facecolor()\n",
    "  # If you want a consistent color, you can just set it as a constant, e.g. #222222\n",
    "  ax.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,\n",
    "          size=12)\n",
    "\n",
    "#plot a line\n",
    "ax.plot(proportion.index,proportion[\"%Fraud\"], color=\"darkgreen\", label=\"Fraud Transactions out of the whole Country Transactions(%)\")\n",
    "ax.set_ylabel(\"Transactions Count (%)\")\n",
    "ax.set_xlabel(\"Country\")\n",
    "    \n",
    "plt.title(\"Fraud vs Genuine Transaction Distribution by Country (%)\")\n",
    "plt.legend(loc=\"best\", prop={'size': 6})\n",
    "plt.figure(figsize=(5, 25))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map countries, based on transactions count\n",
    "\n",
    "# identify number of transactions for each country taking into account shipping address specific\n",
    "map_df=pd.DataFrame(countries).set_index(\"Country\")\n",
    "map_df[\"Fraud Transactions Count\"]=\"\"\n",
    "for i in stacked_df.columns:\n",
    "    map_df.loc[i,\"Fraud Transactions Count\"]=stacked_df[i].sum()\n",
    "    \n",
    "# check coordinates for countries\n",
    "map_df[\"Lat\"]=\"\"\n",
    "map_df[\"Lng\"]=\"\"\n",
    "for i,row in map_df.iterrows():\n",
    "    base_url=\"https://api.geoapify.com/v1/geocode/search\"\n",
    "    params={\n",
    "        \"text\":i,\n",
    "        \"format\":\"json\",\n",
    "        \"apiKey\":geoapify_key\n",
    "    }\n",
    "\n",
    "    data=requests.get(base_url, params=params).json()\n",
    "\n",
    "    # Extract lat/lon\n",
    "    map_df.loc[i,\"Lat\"]=data['results'][0]['lat']\n",
    "    map_df.loc[i,\"Lng\"]=data['results'][0]['lon']\n",
    "map_df=map_df.reset_index()\n",
    "map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4fcc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# be sure we have proper data type for map input\n",
    "map_df[\"Fraud Transactions Count\"]=map_df[\"Fraud Transactions Count\"].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "# Configure the map plot\n",
    "hv.extension('bokeh')\n",
    "map_plot = map_df.hvplot.points(\n",
    "    \"Lng\",\n",
    "    \"Lat\",\n",
    "    geo = True,\n",
    "    tiles = \"OSM\",\n",
    "    frame_width = 500,\n",
    "    frame_height = 360,\n",
    "    size = \"Fraud Transactions Count\",\n",
    "    scale = 0.4,\n",
    "    color=\"Country\"\n",
    ")\n",
    "\n",
    "# Display the map\n",
    "map_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811003e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
